{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AI & Machine Learning (KAN-CINTO4003U) - Copenhagen Business School | Spring 2025**\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part III: LLM\n",
    "\n",
    "Please see the description of the assignment in the README file (section 3) <br>\n",
    "**Guide notebook**: [guides/llm_guide.ipynb](guides/llm_guide.ipynb)\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "<br>\n",
    "\n",
    "* Note that you should report results using a classification report. \n",
    "\n",
    "* Also, remember to include some reflections on your results: how do they compare with the results from Part I, BoW?, and part II, BERT? Are there any hyperparameters or prompting techniques that are particularly important?\n",
    "\n",
    "* You should follow the steps given in the `llm_guide` notebook\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the project\n",
    "from sklearn.metrics import classification_report \n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data\n",
    "\n",
    "We can load this data directly from [Hugging Face Datasets](https://huggingface.co/docs/datasets/) - The HuggingFace Hub- into a Pandas DataFrame. Pretty neat!\n",
    "\n",
    "**Note**: This cell will download the dataset and keep it in memory. If you run this cell multiple times, it will download the dataset multiple times.\n",
    "\n",
    "You are welcome to increase the `frac` parameter to load more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "# train = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])\n",
    "test = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((760, 2),)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "def preprocess(df: pd.DataFrame, frac = 1e-2, label_map = label_map, seed=42) -> pd.DataFrame:\n",
    "    return  (\n",
    "        df\n",
    "        .assign(label=lambda x: x['label'].map(label_map))\n",
    "        [lambda df: df['label'].isin(label_map.values())]\n",
    "        .groupby('label')\n",
    "        .apply(lambda x: x.sample(frac=frac, random_state=seed))\n",
    "        .reset_index(drop=True)\n",
    "\n",
    "    )\n",
    "\n",
    "# train_df = preprocess(train, frac=0.01)\n",
    "test_df = preprocess(test, frac=0.1)\n",
    "\n",
    "# clear up some memory by deleting the original dataframes\n",
    "# del train\n",
    "del test\n",
    "\n",
    "test_df.shape, # train_df.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/mikkel/Library/CloudStorage/OneDrive-Personligt/CBS/Cand Merc IT/2. Semester/AI and ML/mas/ma2\n",
      "Loaded WX_API_KEY: Jl2n9e-WOX7bEQj38Iy2Uj_5gEOuVi7hcFcdTIuhPhN0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 131/760 [01:12<05:41,  1.84it/s]Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-02-19)\n",
      "Status code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"bfa3f18c8edbc1a64286de120f195c62\",\"status_code\":403}\n",
      " 17%|█▋        | 131/760 [01:12<05:49,  1.80it/s]\n"
     ]
    },
    {
     "ename": "ApiRequestFailure",
     "evalue": "Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-02-19)\nStatus code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"bfa3f18c8edbc1a64286de120f195c62\",\"status_code\":403}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mApiRequestFailure\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# generate the response from the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m response = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# extract the generated text from the response\u001b[39;00m\n\u001b[32m    101\u001b[39m prediction = response[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/model_inference.py:617\u001b[39m, in \u001b[36mModelInference.generate\u001b[39m\u001b[34m(self, prompt, params, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit, async_mode, validate_prompt_variables)\u001b[39m\n\u001b[32m    611\u001b[39m     warning_async_mode = (\n\u001b[32m    612\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn this mode, the results will be returned in the order in which the server returns the responses. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    613\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease notice that it does not support non-blocking requests scheduling. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    614\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTo use non-blocking native async inference method you may use `ModelInference.agenerate(...)`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m     )\n\u001b[32m    616\u001b[39m     warn(warning_async_mode)\n\u001b[32m--> \u001b[39m\u001b[32m617\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguardrails\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguardrails_hap_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails_hap_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m    \u001b[49m\u001b[43mguardrails_pii_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails_pii_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidate_prompt_variables\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_prompt_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/fm_model_inference.py:317\u001b[39m, in \u001b[36mFMModelInference.generate\u001b[39m\u001b[34m(self, prompt, params, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit, async_mode, validate_prompt_variables)\u001b[39m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_with_url_async(\n\u001b[32m    307\u001b[39m         prompt=prompt,\n\u001b[32m    308\u001b[39m         params=params \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    313\u001b[39m         concurrency_limit=concurrency_limit,\n\u001b[32m    314\u001b[39m     )\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_url\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerate_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerate_text_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails_hap_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails_hap_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    323\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails_pii_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mguardrails_pii_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    324\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcurrency_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    325\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:746\u001b[39m, in \u001b[36mBaseModelInference._generate_with_url\u001b[39m\u001b[34m(self, prompt, params, generate_url, guardrails, guardrails_hap_params, guardrails_pii_params, concurrency_limit)\u001b[39m\n\u001b[32m    742\u001b[39m             http_client.close()\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_inference_payload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgenerate_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails_hap_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43mguardrails_pii_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/ibm_watsonx_ai/foundation_models/inference/base_model_inference.py:282\u001b[39m, in \u001b[36mBaseModelInference._send_inference_payload\u001b[39m\u001b[34m(self, prompt, params, generate_url, guardrails, guardrails_hap_params, guardrails_pii_params, _http_client)\u001b[39m\n\u001b[32m    278\u001b[39m     post_params[\u001b[33m\"\u001b[39m\u001b[33m_retry_status_codes\u001b[39m\u001b[33m\"\u001b[39m] = requests.DEFAULT_RETRY_STATUS_CODES\n\u001b[32m    280\u001b[39m response_scoring = http_client.post(**post_params)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_scoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_field_to_hide\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerated_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/aiml25-ma2/lib/python3.11/site-packages/ibm_watsonx_ai/wml_resource.py:157\u001b[39m, in \u001b[36mWMLResource._handle_response\u001b[39m\u001b[34m(self, expected_status_code, operationName, response, json_response, _silent_response_logging, _field_to_hide)\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.text\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ApiRequestFailure(\n\u001b[32m    158\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailure during \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(operationName),\n\u001b[32m    159\u001b[39m         response,\n\u001b[32m    160\u001b[39m     )\n",
      "\u001b[31mApiRequestFailure\u001b[39m: Failure during generate. (POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2025-02-19)\nStatus code: 403, body: {\"errors\":[{\"code\":\"token_quota_reached\",\"message\":\"Request of 1 token(s) from quota was rejected\",\"more_info\":\"https://cloud.ibm.com/apidocs/watsonx-ai\"}],\"trace\":\"bfa3f18c8edbc1a64286de120f195c62\",\"status_code\":403}"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "\n",
    "# url = \"https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2023-05-29\"\n",
    "\n",
    "# body = {\n",
    "# \t\"input\": \"\"\"<|assistant|>\n",
    "# \"\"\",\n",
    "# \t\"parameters\": {\n",
    "# \t\t\"decoding_method\": \"greedy\",\n",
    "# \t\t\"max_new_tokens\": 900,\n",
    "# \t\t\"min_new_tokens\": 0,\n",
    "# \t\t\"repetition_penalty\": 1.05\n",
    "# \t},\n",
    "# \t\"model_id\": \"ibm/granite-13b-instruct-v2\",\n",
    "# \t\"project_id\": \"883cb7f1-0849-4a0c-ae59-e89def58b89b\"\n",
    "# }\n",
    "\n",
    "# headers = {\n",
    "# \t\"Accept\": \"application/json\",\n",
    "# \t\"Content-Type\": \"application/json\",\n",
    "# \t\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\"\n",
    "# }\n",
    "\n",
    "# response = requests.post(\n",
    "# \turl,\n",
    "# \theaders=headers,\n",
    "# \tjson=body\n",
    "# )\n",
    "\n",
    "# if response.status_code != 200:\n",
    "# \traise Exception(\"Non-200 response: \" + str(response.text))\n",
    "\n",
    "# data = response.json()\n",
    "# Setup watsonx\n",
    "from decouple import Config\n",
    "from ibm_watsonx_ai import APIClient\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import ModelInference\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure the working directory is set to the \"ma2\" folder.\n",
    "while Path.cwd().name != \"ma2\" and \"ma2\" in str(Path.cwd()):\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")\n",
    "\n",
    "config = Config('.env')\n",
    "WX_API_KEY = config('WX_API_KEY')\n",
    "print(f\"Loaded WX_API_KEY: {WX_API_KEY}\")\n",
    "credentials = Credentials(\n",
    "                url = \"https://us-south.ml.cloud.ibm.com\",\n",
    "                api_key = WX_API_KEY\n",
    "                )\n",
    "\n",
    "client = APIClient(\n",
    "                credentials=credentials, \n",
    "                project_id=\"883cb7f1-0849-4a0c-ae59-e89def58b89b\"\n",
    "                )\n",
    "\n",
    "model = ModelInference(\n",
    "    api_client=client,\n",
    "    model_id=\"ibm/granite-13b-instruct-v2\",\n",
    ")\n",
    "prompt = \"How do I make a cake?\"\n",
    "generated_response = model.generate(prompt)\n",
    "\n",
    "generated_response\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Your task is to classify news stories into one of four categories.\n",
    "\n",
    "CATEGORIES:\n",
    "0: World — News about international events, global politics, diplomacy, and major world affairs.  \n",
    "1: Sports — News about sports events, teams, athletes, matches, or sports-related activities.  \n",
    "2: Business — News about companies, markets, economics, financial updates, or industry trends.  \n",
    "3: Sci/Tech — News about science, technology, innovations, research, or breakthroughs in these fields.  \n",
    "\n",
    "TEXT:\n",
    "{text}\n",
    "\n",
    "Please assign the correct category to the text. Answer with the correct category and nothing else.\n",
    "\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "CATEGORIES = \"- \" + \"\\n- \".join(test_df[\"label\"].unique())  # Create a string with all categories\n",
    "\n",
    "predictions = []\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for text in tqdm(test_df[\"text\"]):\n",
    "\n",
    "    # format the prompt with the categories and the text\n",
    "    prompt = SYSTEM_PROMPT.format(categories=CATEGORIES, text=text)\n",
    "    \n",
    "    # generate the response from the model\n",
    "    response = model.generate(prompt)\n",
    "\n",
    "    # extract the generated text from the response\n",
    "    prediction = response[\"results\"][0][\"generated_text\"].strip()\n",
    "\n",
    "    # append the prediction to the list of predictions\n",
    "    predictions.append(prediction)\n",
    "    \n",
    "    \n",
    "print(classification_report(test_df.label, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran out of Tokens but I would have tried different system prompts in order to have the model classify the data better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
